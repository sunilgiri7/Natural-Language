# -*- coding: utf-8 -*-
"""transformer & Bert implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m558sqHswZTKVxKHK7n_f2bm_gFKITB9
"""

# ! pip install git+https://github.com/huggingface/transformers.git

"""# Quick tour
Let's have a quick look at the ðŸ¤— Transformers library features. The library downloads pretrained models for Natural Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG), such as completing a prompt with new text or translating in another language.

First we will see how to easily leverage the pipeline API to quickly use those pretrained models at inference. Then, we will dig a little bit more and see how the library gives you access to those models and helps you preprocess your data.

# Getting started on a task with a pipeline
### The easiest way to use a pretrained model on a given task is to use pipeline. ðŸ¤— Transformers provides the following tasks out of the box:

1. Sentiment analysis: is a text positive or negative?
Text generation (in English): provide a prompt and the model will generate what follows.
2. Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)
3. Question answering: provide the model with some context and a question, extract the answer from the context.
4. Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.
5. Summarization: generate a summary of a long text.
Translation: translate a text in another language.
6. Feature extraction: return a tensor representation of the text.
"""

# This pipeline actually call pretrained model of transformer
from transformers import pipeline
classifier = pipeline('sentiment-analysis')

"""When typing this command for the first time, a pretrained model and its tokenizer are downloaded and cached. We will look at both later on, but as an introduction the tokenizer's job is to preprocess the text for the model, which is then responsible for making predictions. The pipeline groups all of that together, and post-process the predictions to make them readable. For instance: bold text"""

classifier('We are very happy to show you the ðŸ¤— Transformers library.')

result = classifier(["We are very happy to show you the ðŸ¤— Transformers library.",
           "We hope you don't hate it."])
for token in result:
  print(f"labels:{token['label']}, with score:{round(token['score'], 4)}")

"""You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is fairly neutral.

By default, the model downloaded for this pipeline is called "distilbert-base-uncased-finetuned-sst-2-english". We can look at its model page to get more information about it. It uses the DistilBERT architecture and has been fine-tuned on a dataset called SST-2 for the sentiment analysis task.

Let's say we want to use another model; for instance, one that has been trained on French data. We can search through the model hub that gathers models pretrained on a lot of data by research labs, but also community models (usually fine-tuned versions of those big models on a specific dataset). Applying the tags "French" and "text-classification" gives back a suggestion "nlptown/bert-base-multilingual-uncased-sentiment". Let's see how we can use it.

You can directly pass the name of the model to use to pipeline:
"""

classifier = pipeline('sentiment-analysis',  model="nlptown/bert-base-multilingual-uncased-sentiment")

classifier('comment allez-vous les gars ')

"""This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! You can also replace that name by a local folder where you have saved a pretrained model (see below). You can also pass a model object and its associated tokenizer.

We will need two classes for this. The first is AutoTokenizer, which we will use to download the tokenizer associated to the model we picked and instantiate it. The second is AutoModelForSequenceClassification (or TFAutoModelForSequenceClassification if you are using TensorFlow), which we will use to download the model itself. Note that if we were using the library on an other task, the class of the model would change. The task summary tutorial summarizes which class is used for which task.
"""

from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = "nlptown/bert-base-multilingual-uncased-sentiment"
# This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.
models = TFAutoModelForSequenceClassification.from_pretrained(model, from_pt=True)
tokenizer = AutoTokenizer.from_pretrained(model)
classifier = pipeline('sentiment-analysis', model = model, tokenizer=tokenizer)

classifier('you are good boy')

"""Let's now see what happens beneath the hood when using those pipelines. As we saw, the model and tokenizer are created using the from_pretrained method:"""

from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""# Using the tokenizer

We mentioned the tokenizer is responsible for the preprocessing of your texts. First, it will split a given text in
words (or part of words, punctuation symbols, etc.) usually called *tokens*. There are multiple rules that can govern
that process (you can learn more about them in the [tokenizer summary](https://huggingface.co/transformers/tokenizer_summary.html)), which is why we need
to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was
pretrained.

The second step is to convert those *tokens* into numbers, to be able to build a tensor out of them and feed them to
the model. To do this, the tokenizer has a *vocab*, which is the part we download when we instantiate it with the
`from_pretrained` method, since we need to use the same *vocab* as when the model was pretrained.

To apply these steps on a given text, we can just feed it to our tokenizer:
"""

input = tokenizer('We are very happy to show you the ðŸ¤— Transformers library.')

#this work like word2vec embedding technique these are the index value of particular word
print(input)

# to fix the index size we basically do
tf_input = tokenizer(['We are very happy to show you the ðŸ¤— Transformers library.', 'what is your name'],
                     padding=True,
                     truncation=True,
                     max_length = 500,
                     return_tensors='tf')

tf_input

# this is just pretrained model of hugging face  to find answer of particular questions
from transformers import pipeline

question_answerer = pipeline(task="question-answering")
preds = question_answerer(
    question="What is the name of the repository?",
    context="The name of the repository is huggingface/transformers",
)
print(
    f"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}"
)

