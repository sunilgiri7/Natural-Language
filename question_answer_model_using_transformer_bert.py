# -*- coding: utf-8 -*-
"""Question Answer Model Using Transformer BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xujLoH1hQNjq_NJksnwc3QvU502AoVzR
"""


import json
with open(r"qsnAnswer.json", 'r') as f:
  train = json.load(f)

with open(r'test_qsnAnswer.json', 'r') as f:
  test = json.load(f)

train_data = []
for example in train['data']:
    context = example['paragraphs'][0]['context']
    qas = example['paragraphs'][0]['qas']
    for qa in qas:
        qid = qa['id']
        question = qa['question']
        answer = qa['answers'][0]['text']
        answer_start = qa['answers'][0]['answer_start']
        train_data.append({'context': context, 'qas': [{'question': question, 'id': qid, 'answers': [{'text': answer, 'answer_start': answer_start}]}]})
        
test_data = []
for example in test['data']:
    context = example['paragraphs'][0]['context']
    qas = example['paragraphs'][0]['qas']
    for qa in qas:
        qid = qa['id']
        question = qa['question']
        answer = qa['answers'][0]['text']
        answer_start = qa['answers'][0]['answer_start']
        test_data.append({'context': context, 'qas': [{'question': question, 'id': qid, 'answers': [{'text': answer, 'answer_start': answer_start}]}]})

test_data


import logging
from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs

# It train on model type and model name of hugging face 
#it will be give random model from any of these so we give if else condition
model_type="bert"
model_name= "bert-base-cased"
if model_type == "bert":
    model_name = "bert-base-cased"

elif model_type == "roberta":
    model_name = "roberta-base"

elif model_type == "distilbert":
    model_name = "distilbert-base-cased"

elif model_type == "distilroberta":
    model_type = "roberta"
    model_name = "distilroberta-base"

elif model_type == "electra-base":
    model_type = "electra"
    model_name = "google/electra-base-discriminator"

elif model_type == "electra-small":
    model_type = "electra"
    model_name = "google/electra-small-discriminator"

elif model_type == "xlnet":
    model_name = "xlnet-based-cased"

# now when ever we take any model we also can setup parameter for that
model_args = QuestionAnsweringArgs(n_best_size=10)
model_args.train_batch_size = 32
model_args.evaluate_during_training = True
model_args.n_best_size = 3
model_args.num_train_epochs = 20

# now these are the all key value pair of model iam selecting
### Advanced Methodology
train_args = {
    "reprocess_input_data": True,
    "overwrite_output_dir": True,
    "use_cached_eval_features": True,
    "output_dir": f"outputs/{model_type}",
    "best_model_dir": f"outputs/{model_type}/best_model",
    "evaluate_during_training": True,
    "max_seq_length": 128,
    "num_train_epochs": 20,
    "evaluate_during_training_steps": 1000,
    "wandb_project": "Question Answer Application",
    "wandb_kwargs": {"name": model_name},
    "save_model_every_epoch": False,
    "save_eval_checkpoints": False,
    "n_best_size":3,
    "use_early_stopping": True,
    # "early_stopping_metric": "mcc",
    # "n_gpu": 2,
    # "manual_seed": 4,
    # "use_multiprocessing": False,
    "train_batch_size": 128,
    "eval_batch_size": 64,
    # "config": {
    #     "output_hidden_states": True
    # }
}

model = QuestionAnsweringModel(
    model_type, model_name, args=train_args,use_cuda=False
)


model.train_model(train_data, eval_data = test_data)

result, texts = model.eval_model(test_data)

result

to_predict = [{"context": "Vin is a Mistborn of great power and skill.", 
               "qas": [{"question": "What is Vin's speciality?", "id": "0"}]}]

answer, probabilities =  model.predict(to_predict)
answer = answer[0]['answer']

" ".join(answer)